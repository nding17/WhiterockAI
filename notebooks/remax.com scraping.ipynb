{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "city = 'Philadelphia'\n",
    "state = 'PA'\n",
    "overhead = 'https://www.remax.com/realestatehomesforsale'\n",
    "url = 'https://www.remax.com/realestatehomesforsale/philadelphia-pa-p002.html?query=philadelphia,pa-search/newest-sortorder'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_webpage(city, state, pg_num):\n",
    "    city = city.strip().lower()\n",
    "    state = state.strip().lower()\n",
    "    url = f'{overhead}/{city}-{state}-p{pg_num}.html?query={city},{state}-search/newest-sortorder'\n",
    "    return url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://www.remax.com/realestatehomesforsale/philadelphia-pa-p1.html?query=philadelphia,pa-search/newest-sortorder'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_webpage('philadelphia', 'PA', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_apt_urls_per_page(city, state, pg_num):\n",
    "    webpage = get_webpage(city, state, pg_num)\n",
    "    response = requests.get(webpage)\n",
    "    results = response.content\n",
    "    apt_urls = []\n",
    "    \n",
    "    if not response.status_code == 404:\n",
    "        soup = BeautifulSoup(results, 'lxml')\n",
    "        apt_sub_tags = soup.find_all('div', class_='listing-pane-details')\n",
    "        \n",
    "        for apt_tag in apt_sub_tags:\n",
    "            apt_link_tag = apt_tag.find('a', class_='js-detaillink')\n",
    "            url = apt_link_tag['href']\n",
    "            apt_urls.append(url)\n",
    "        \n",
    "    return apt_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/realestatehomesforsale/1417-n-8th-st-philadelphia-pa-19122-id342778675.html',\n",
       " '/realestatehomesforsale/767-n-24th-st-philadelphia-pa-19130-gid400025159539.html',\n",
       " '/realestatehomesforsale/2211-moore-st-philadelphia-pa-19145-id342779562.html',\n",
       " '/realestatehomesforsale/763-n-bucknell-st-philadelphia-pa-19130-gid400025239517.html',\n",
       " '/realestatehomesforsale/1121-lemon-st-philadelphia-pa-19123-gid400030275230.html',\n",
       " '/realestatehomesforsale/1438-s-8th-st-philadelphia-pa-19147-gid400025277879.html',\n",
       " '/realestatehomesforsale/2519-2521-n-front-st-philadelphia-pa-19133-id342779336.html',\n",
       " '/realestatehomesforsale/1606-s-11th-st-philadelphia-pa-19148-id342779667.html',\n",
       " '/realestatehomesforsale/2017-s-6th-st-philadelphia-pa-19148-gid400025375118.html',\n",
       " '/realestatehomesforsale/1532-n-7th-st-no-2-philadelphia-pa-19122-id342779177.html',\n",
       " '/realestatehomesforsale/1234-s-7th-st-philadelphia-pa-19147-gid400025200829.html',\n",
       " '/realestatehomesforsale/1222-emily-st-philadelphia-pa-19148-gid400025373671.html',\n",
       " '/realestatehomesforsale/7618-langdon-st-philadelphia-pa-19111-id342743093.html',\n",
       " '/realestatehomesforsale/2309-cross-st-philadelphia-pa-19146-id342779295.html',\n",
       " '/realestatehomesforsale/1719-kendrick-st-philadelphia-pa-19152-gid400033903901.html',\n",
       " '/realestatehomesforsale/1338-n-dover-st-philadelphia-pa-19121-gid400025363463.html',\n",
       " '/realestatehomesforsale/9301-campus-ln-philadelphia-pa-19114-gid400025474253.html',\n",
       " '/realestatehomesforsale/11112-ridgeway-st-philadelphia-pa-19116-gid400033909436.html',\n",
       " '/realestatehomesforsale/1630-90-welsh-rd-no-20c-philadelphia-pa-19115-gid400025569379.html',\n",
       " '/realestatehomesforsale/2517-w-seybert-st-philadelphia-pa-19121-gid400025348302.html',\n",
       " '/realestatehomesforsale/444-w-bringhurst-st-philadelphia-pa-19144-id342778783.html',\n",
       " '/realestatehomesforsale/4407-cottman-ave-philadelphia-pa-19135-id342778949.html',\n",
       " '/realestatehomesforsale/2602-s-6th-st-philadelphia-pa-19148-gid400025531280.html',\n",
       " '/realestatehomesforsale/3211-atmore-rd-philadelphia-pa-19154-gid400025491214.html']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_apt_urls_per_page('philadelphia', 'PA', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ensemble_apt_urls(city, state, verbose=False):\n",
    "    test_page = get_webpage(city, state, 1)\n",
    "    response = requests.get(test_page)\n",
    "    results = response.content\n",
    "    apt_ensemble_urls = []\n",
    "    \n",
    "    if not response.status_code == 404:\n",
    "        soup = BeautifulSoup(results, 'lxml')\n",
    "        pg_lst = soup.find_all('li', class_='pages-item')\n",
    "        try:\n",
    "            max_pg_tag = pg_lst[-1].find('a', class_='js-pager-item pages-link')\n",
    "            max_pg = int(max_pg_tag.get_text())\n",
    "            if verbose:\n",
    "                print(f'there are {max_pg} apartment URLs to be collected')\n",
    "        except:\n",
    "            max_pg = np.nan\n",
    "        \n",
    "        if not max_pg == np.nan:\n",
    "            for pg_num in range(1, max_pg+1):\n",
    "                apt_ensemble_urls += get_apt_urls_per_page(city, state, pg_num)\n",
    "                if verbose:\n",
    "                    print(f'page {pg_num} apartment URLs collected')\n",
    "        if verbose:\n",
    "            print(f'all apartment URLs collected')\n",
    "    return apt_ensemble_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 394 apartment URLs to be collected\n",
      "page 1 apartment URLs collected\n",
      "page 2 apartment URLs collected\n",
      "page 3 apartment URLs collected\n",
      "page 4 apartment URLs collected\n",
      "page 5 apartment URLs collected\n",
      "page 6 apartment URLs collected\n",
      "page 7 apartment URLs collected\n",
      "page 8 apartment URLs collected\n",
      "page 9 apartment URLs collected\n",
      "page 10 apartment URLs collected\n",
      "page 11 apartment URLs collected\n",
      "page 12 apartment URLs collected\n",
      "page 13 apartment URLs collected\n",
      "page 14 apartment URLs collected\n",
      "page 15 apartment URLs collected\n",
      "page 16 apartment URLs collected\n",
      "page 17 apartment URLs collected\n",
      "page 18 apartment URLs collected\n",
      "page 19 apartment URLs collected\n",
      "page 20 apartment URLs collected\n",
      "page 21 apartment URLs collected\n",
      "page 22 apartment URLs collected\n",
      "page 23 apartment URLs collected\n",
      "page 24 apartment URLs collected\n",
      "page 25 apartment URLs collected\n",
      "page 26 apartment URLs collected\n",
      "page 27 apartment URLs collected\n",
      "page 28 apartment URLs collected\n",
      "page 29 apartment URLs collected\n",
      "page 30 apartment URLs collected\n",
      "page 31 apartment URLs collected\n",
      "page 32 apartment URLs collected\n",
      "page 33 apartment URLs collected\n",
      "page 34 apartment URLs collected\n",
      "page 35 apartment URLs collected\n",
      "page 36 apartment URLs collected\n",
      "page 37 apartment URLs collected\n",
      "page 38 apartment URLs collected\n",
      "page 39 apartment URLs collected\n",
      "page 40 apartment URLs collected\n",
      "page 41 apartment URLs collected\n",
      "page 42 apartment URLs collected\n",
      "page 43 apartment URLs collected\n",
      "page 44 apartment URLs collected\n",
      "page 45 apartment URLs collected\n",
      "page 46 apartment URLs collected\n",
      "page 47 apartment URLs collected\n",
      "page 48 apartment URLs collected\n",
      "page 49 apartment URLs collected\n",
      "page 50 apartment URLs collected\n",
      "page 51 apartment URLs collected\n",
      "page 52 apartment URLs collected\n",
      "page 53 apartment URLs collected\n",
      "page 54 apartment URLs collected\n",
      "page 55 apartment URLs collected\n",
      "page 56 apartment URLs collected\n",
      "page 57 apartment URLs collected\n",
      "page 58 apartment URLs collected\n",
      "page 59 apartment URLs collected\n",
      "page 60 apartment URLs collected\n",
      "page 61 apartment URLs collected\n",
      "page 62 apartment URLs collected\n",
      "page 63 apartment URLs collected\n",
      "page 64 apartment URLs collected\n",
      "page 65 apartment URLs collected\n",
      "page 66 apartment URLs collected\n",
      "page 67 apartment URLs collected\n",
      "page 68 apartment URLs collected\n",
      "page 69 apartment URLs collected\n",
      "page 70 apartment URLs collected\n",
      "page 71 apartment URLs collected\n",
      "page 72 apartment URLs collected\n",
      "page 73 apartment URLs collected\n",
      "page 74 apartment URLs collected\n",
      "page 75 apartment URLs collected\n",
      "page 76 apartment URLs collected\n",
      "page 77 apartment URLs collected\n",
      "page 78 apartment URLs collected\n",
      "page 79 apartment URLs collected\n",
      "page 80 apartment URLs collected\n",
      "page 81 apartment URLs collected\n",
      "page 82 apartment URLs collected\n",
      "page 83 apartment URLs collected\n",
      "page 84 apartment URLs collected\n",
      "page 85 apartment URLs collected\n",
      "page 86 apartment URLs collected\n",
      "page 87 apartment URLs collected\n",
      "page 88 apartment URLs collected\n",
      "page 89 apartment URLs collected\n",
      "page 90 apartment URLs collected\n",
      "page 91 apartment URLs collected\n",
      "page 92 apartment URLs collected\n",
      "page 93 apartment URLs collected\n",
      "page 94 apartment URLs collected\n",
      "page 95 apartment URLs collected\n",
      "page 96 apartment URLs collected\n",
      "page 97 apartment URLs collected\n",
      "page 98 apartment URLs collected\n",
      "page 99 apartment URLs collected\n",
      "page 100 apartment URLs collected\n",
      "page 101 apartment URLs collected\n",
      "page 102 apartment URLs collected\n",
      "page 103 apartment URLs collected\n",
      "page 104 apartment URLs collected\n",
      "page 105 apartment URLs collected\n",
      "page 106 apartment URLs collected\n",
      "page 107 apartment URLs collected\n",
      "page 108 apartment URLs collected\n",
      "page 109 apartment URLs collected\n",
      "page 110 apartment URLs collected\n",
      "page 111 apartment URLs collected\n",
      "page 112 apartment URLs collected\n",
      "page 113 apartment URLs collected\n",
      "page 114 apartment URLs collected\n",
      "page 115 apartment URLs collected\n",
      "page 116 apartment URLs collected\n",
      "page 117 apartment URLs collected\n",
      "page 118 apartment URLs collected\n",
      "page 119 apartment URLs collected\n",
      "page 120 apartment URLs collected\n",
      "page 121 apartment URLs collected\n",
      "page 122 apartment URLs collected\n",
      "page 123 apartment URLs collected\n",
      "page 124 apartment URLs collected\n",
      "page 125 apartment URLs collected\n",
      "page 126 apartment URLs collected\n",
      "page 127 apartment URLs collected\n",
      "page 128 apartment URLs collected\n",
      "page 129 apartment URLs collected\n",
      "page 130 apartment URLs collected\n",
      "page 131 apartment URLs collected\n",
      "page 132 apartment URLs collected\n",
      "page 133 apartment URLs collected\n",
      "page 134 apartment URLs collected\n",
      "page 135 apartment URLs collected\n",
      "page 136 apartment URLs collected\n",
      "page 137 apartment URLs collected\n",
      "page 138 apartment URLs collected\n",
      "page 139 apartment URLs collected\n",
      "page 140 apartment URLs collected\n",
      "page 141 apartment URLs collected\n",
      "page 142 apartment URLs collected\n",
      "page 143 apartment URLs collected\n",
      "page 144 apartment URLs collected\n",
      "page 145 apartment URLs collected\n",
      "page 146 apartment URLs collected\n",
      "page 147 apartment URLs collected\n",
      "page 148 apartment URLs collected\n",
      "page 149 apartment URLs collected\n",
      "page 150 apartment URLs collected\n",
      "page 151 apartment URLs collected\n",
      "page 152 apartment URLs collected\n",
      "page 153 apartment URLs collected\n",
      "page 154 apartment URLs collected\n",
      "page 155 apartment URLs collected\n",
      "page 156 apartment URLs collected\n",
      "page 157 apartment URLs collected\n",
      "page 158 apartment URLs collected\n",
      "page 159 apartment URLs collected\n",
      "page 160 apartment URLs collected\n",
      "page 161 apartment URLs collected\n",
      "page 162 apartment URLs collected\n",
      "page 163 apartment URLs collected\n",
      "page 164 apartment URLs collected\n",
      "page 165 apartment URLs collected\n",
      "page 166 apartment URLs collected\n",
      "page 167 apartment URLs collected\n",
      "page 168 apartment URLs collected\n",
      "page 169 apartment URLs collected\n",
      "page 170 apartment URLs collected\n",
      "page 171 apartment URLs collected\n",
      "page 172 apartment URLs collected\n",
      "page 173 apartment URLs collected\n",
      "page 174 apartment URLs collected\n",
      "page 175 apartment URLs collected\n",
      "page 176 apartment URLs collected\n",
      "page 177 apartment URLs collected\n",
      "page 178 apartment URLs collected\n",
      "page 179 apartment URLs collected\n",
      "page 180 apartment URLs collected\n",
      "page 181 apartment URLs collected\n",
      "page 182 apartment URLs collected\n",
      "page 183 apartment URLs collected\n",
      "page 184 apartment URLs collected\n",
      "page 185 apartment URLs collected\n",
      "page 186 apartment URLs collected\n",
      "page 187 apartment URLs collected\n",
      "page 188 apartment URLs collected\n",
      "page 189 apartment URLs collected\n",
      "page 190 apartment URLs collected\n",
      "page 191 apartment URLs collected\n",
      "page 192 apartment URLs collected\n",
      "page 193 apartment URLs collected\n",
      "page 194 apartment URLs collected\n",
      "page 195 apartment URLs collected\n",
      "page 196 apartment URLs collected\n",
      "page 197 apartment URLs collected\n",
      "page 198 apartment URLs collected\n",
      "page 199 apartment URLs collected\n",
      "page 200 apartment URLs collected\n",
      "page 201 apartment URLs collected\n",
      "page 202 apartment URLs collected\n",
      "page 203 apartment URLs collected\n",
      "page 204 apartment URLs collected\n",
      "page 205 apartment URLs collected\n",
      "page 206 apartment URLs collected\n",
      "page 207 apartment URLs collected\n",
      "page 208 apartment URLs collected\n",
      "page 209 apartment URLs collected\n",
      "page 210 apartment URLs collected\n",
      "page 211 apartment URLs collected\n",
      "page 212 apartment URLs collected\n",
      "page 213 apartment URLs collected\n",
      "page 214 apartment URLs collected\n",
      "page 215 apartment URLs collected\n",
      "page 216 apartment URLs collected\n",
      "page 217 apartment URLs collected\n",
      "page 218 apartment URLs collected\n",
      "page 219 apartment URLs collected\n",
      "page 220 apartment URLs collected\n",
      "page 221 apartment URLs collected\n",
      "page 222 apartment URLs collected\n",
      "page 223 apartment URLs collected\n",
      "page 224 apartment URLs collected\n",
      "page 225 apartment URLs collected\n",
      "page 226 apartment URLs collected\n",
      "page 227 apartment URLs collected\n",
      "page 228 apartment URLs collected\n",
      "page 229 apartment URLs collected\n",
      "page 230 apartment URLs collected\n",
      "page 231 apartment URLs collected\n",
      "page 232 apartment URLs collected\n",
      "page 233 apartment URLs collected\n",
      "page 234 apartment URLs collected\n",
      "page 235 apartment URLs collected\n",
      "page 236 apartment URLs collected\n",
      "page 237 apartment URLs collected\n",
      "page 238 apartment URLs collected\n",
      "page 239 apartment URLs collected\n",
      "page 240 apartment URLs collected\n",
      "page 241 apartment URLs collected\n",
      "page 242 apartment URLs collected\n",
      "page 243 apartment URLs collected\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page 244 apartment URLs collected\n",
      "page 245 apartment URLs collected\n",
      "page 246 apartment URLs collected\n",
      "page 247 apartment URLs collected\n",
      "page 248 apartment URLs collected\n",
      "page 249 apartment URLs collected\n",
      "page 250 apartment URLs collected\n",
      "page 251 apartment URLs collected\n",
      "page 252 apartment URLs collected\n",
      "page 253 apartment URLs collected\n",
      "page 254 apartment URLs collected\n",
      "page 255 apartment URLs collected\n",
      "page 256 apartment URLs collected\n",
      "page 257 apartment URLs collected\n",
      "page 258 apartment URLs collected\n",
      "page 259 apartment URLs collected\n",
      "page 260 apartment URLs collected\n",
      "page 261 apartment URLs collected\n",
      "page 262 apartment URLs collected\n",
      "page 263 apartment URLs collected\n",
      "page 264 apartment URLs collected\n",
      "page 265 apartment URLs collected\n",
      "page 266 apartment URLs collected\n",
      "page 267 apartment URLs collected\n",
      "page 268 apartment URLs collected\n",
      "page 269 apartment URLs collected\n",
      "page 270 apartment URLs collected\n",
      "page 271 apartment URLs collected\n",
      "page 272 apartment URLs collected\n",
      "page 273 apartment URLs collected\n",
      "page 274 apartment URLs collected\n",
      "page 275 apartment URLs collected\n",
      "page 276 apartment URLs collected\n",
      "page 277 apartment URLs collected\n",
      "page 278 apartment URLs collected\n",
      "page 279 apartment URLs collected\n",
      "page 280 apartment URLs collected\n",
      "page 281 apartment URLs collected\n",
      "page 282 apartment URLs collected\n",
      "page 283 apartment URLs collected\n",
      "page 284 apartment URLs collected\n",
      "page 285 apartment URLs collected\n",
      "page 286 apartment URLs collected\n",
      "page 287 apartment URLs collected\n",
      "page 288 apartment URLs collected\n",
      "page 289 apartment URLs collected\n",
      "page 290 apartment URLs collected\n",
      "page 291 apartment URLs collected\n",
      "page 292 apartment URLs collected\n",
      "page 293 apartment URLs collected\n",
      "page 294 apartment URLs collected\n",
      "page 295 apartment URLs collected\n",
      "page 296 apartment URLs collected\n",
      "page 297 apartment URLs collected\n",
      "page 298 apartment URLs collected\n",
      "page 299 apartment URLs collected\n",
      "page 300 apartment URLs collected\n",
      "page 301 apartment URLs collected\n",
      "page 302 apartment URLs collected\n",
      "page 303 apartment URLs collected\n",
      "page 304 apartment URLs collected\n",
      "page 305 apartment URLs collected\n",
      "page 306 apartment URLs collected\n",
      "page 307 apartment URLs collected\n",
      "page 308 apartment URLs collected\n",
      "page 309 apartment URLs collected\n",
      "page 310 apartment URLs collected\n",
      "page 311 apartment URLs collected\n",
      "page 312 apartment URLs collected\n",
      "page 313 apartment URLs collected\n",
      "page 314 apartment URLs collected\n",
      "page 315 apartment URLs collected\n",
      "page 316 apartment URLs collected\n",
      "page 317 apartment URLs collected\n",
      "page 318 apartment URLs collected\n",
      "page 319 apartment URLs collected\n",
      "page 320 apartment URLs collected\n",
      "page 321 apartment URLs collected\n",
      "page 322 apartment URLs collected\n",
      "page 323 apartment URLs collected\n",
      "page 324 apartment URLs collected\n",
      "page 325 apartment URLs collected\n",
      "page 326 apartment URLs collected\n",
      "page 327 apartment URLs collected\n",
      "page 328 apartment URLs collected\n",
      "page 329 apartment URLs collected\n",
      "page 330 apartment URLs collected\n",
      "page 331 apartment URLs collected\n",
      "page 332 apartment URLs collected\n",
      "page 333 apartment URLs collected\n",
      "page 334 apartment URLs collected\n",
      "page 335 apartment URLs collected\n",
      "page 336 apartment URLs collected\n",
      "page 337 apartment URLs collected\n",
      "page 338 apartment URLs collected\n",
      "page 339 apartment URLs collected\n",
      "page 340 apartment URLs collected\n",
      "page 341 apartment URLs collected\n",
      "page 342 apartment URLs collected\n",
      "page 343 apartment URLs collected\n",
      "page 344 apartment URLs collected\n",
      "page 345 apartment URLs collected\n",
      "page 346 apartment URLs collected\n",
      "page 347 apartment URLs collected\n",
      "page 348 apartment URLs collected\n",
      "page 349 apartment URLs collected\n",
      "page 350 apartment URLs collected\n",
      "page 351 apartment URLs collected\n",
      "page 352 apartment URLs collected\n",
      "page 353 apartment URLs collected\n",
      "page 354 apartment URLs collected\n",
      "page 355 apartment URLs collected\n",
      "page 356 apartment URLs collected\n",
      "page 357 apartment URLs collected\n",
      "page 358 apartment URLs collected\n",
      "page 359 apartment URLs collected\n",
      "page 360 apartment URLs collected\n",
      "page 361 apartment URLs collected\n",
      "page 362 apartment URLs collected\n",
      "page 363 apartment URLs collected\n",
      "page 364 apartment URLs collected\n",
      "page 365 apartment URLs collected\n",
      "page 366 apartment URLs collected\n",
      "page 367 apartment URLs collected\n",
      "page 368 apartment URLs collected\n",
      "page 369 apartment URLs collected\n",
      "page 370 apartment URLs collected\n",
      "page 371 apartment URLs collected\n",
      "page 372 apartment URLs collected\n",
      "page 373 apartment URLs collected\n",
      "page 374 apartment URLs collected\n",
      "page 375 apartment URLs collected\n",
      "page 376 apartment URLs collected\n",
      "page 377 apartment URLs collected\n",
      "page 378 apartment URLs collected\n",
      "page 379 apartment URLs collected\n",
      "page 380 apartment URLs collected\n",
      "page 381 apartment URLs collected\n",
      "page 382 apartment URLs collected\n",
      "page 383 apartment URLs collected\n",
      "page 384 apartment URLs collected\n",
      "page 385 apartment URLs collected\n",
      "page 386 apartment URLs collected\n",
      "page 387 apartment URLs collected\n",
      "page 388 apartment URLs collected\n",
      "page 389 apartment URLs collected\n",
      "page 390 apartment URLs collected\n",
      "page 391 apartment URLs collected\n",
      "page 392 apartment URLs collected\n",
      "page 393 apartment URLs collected\n",
      "page 394 apartment URLs collected\n",
      "all apartment URLs collected\n"
     ]
    }
   ],
   "source": [
    "all_apt_urls = get_ensemble_apt_urls('philadelphia', 'pa', verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/realestatehomesforsale/1417-n-8th-st-philadelphia-pa-19122-id342778675.html'"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collection_url = all_apt_urls[0]\n",
    "collection_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/realestatehomesforsale/767-n-24th-st-philadelphia-pa-19130-gid400025159539.html'"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normal_url = all_apt_urls[1]\n",
    "normal_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_price(soup):\n",
    "    try:\n",
    "        price_tag = soup.find('span', class_='listing-detail-price-amount pad-half-right')\n",
    "        price_text = price_tag.get_text()\\\n",
    "                          .replace(',','')\\\n",
    "                          .strip()\n",
    "        pattern = r'[-+]?\\d*\\.\\d+|\\d+'\n",
    "        price_unit = re.findall(pattern, price_text)[0]\n",
    "        price = float(price_unit)\n",
    "\n",
    "        return price\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "def get_address(content_tag):\n",
    "    try:\n",
    "        address_tag = content_tag.find('div', class_='listing-detail-address')\n",
    "        street_tag = address_tag.find('span', attrs={'itemprop': 'streetAddress'})\n",
    "        street = street_tag.get_text()\\\n",
    "                           .strip()\\\n",
    "                           .replace(',', '')\n",
    "        city_tag = address_tag.find('span', attrs={'itemprop': 'addressLocality'})\n",
    "        city = city_tag.get_text()\\\n",
    "                       .strip()\\\n",
    "                       .replace(',', '')\\\n",
    "                       .title()\n",
    "        state_tag = address_tag.find('span', attrs={'itemprop': 'addressRegion'})\n",
    "        state = state_tag.get_text()\\\n",
    "                         .strip()\n",
    "        zipcode_tag = address_tag.find('span', attrs={'itemprop': 'postalCode'})\n",
    "        zipcode = zipcode_tag.get_text()\\\n",
    "                             .strip()\n",
    "        \n",
    "        return street, city, state, zipcode\n",
    "    \n",
    "    except:\n",
    "        return None, None, None, None\n",
    "    \n",
    "def get_sideinfo(content_tag):\n",
    "    sideinfo = {}\n",
    "    try:\n",
    "        apt_info_tag = content_tag.find('div', class_='forsalelistingdetail')\n",
    "        apt_list_tag = apt_info_tag.find_all('li', class_='listing-detail-stats')\n",
    "        \n",
    "        for apt_tag in apt_list_tag:\n",
    "            spans = apt_tag.find_all('span')\n",
    "            key = spans[0].get_text()\\\n",
    "                          .strip()\n",
    "            value = spans[1].get_text()\\\n",
    "                            .strip()\n",
    "            sideinfo[key] = value\n",
    "        return sideinfo\n",
    "    except:\n",
    "        return sideinfo\n",
    "\n",
    "def access_dict(d, key):\n",
    "    try:\n",
    "        value = d[key]\n",
    "        if 'sqft' in value:\n",
    "            value = value.replace(',','')\\\n",
    "                         .replace('sqft', '')\\\n",
    "                         .strip()\n",
    "        try:\n",
    "            return float(value)\n",
    "        except: \n",
    "            return value\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remax_normal(soup):\n",
    "    # REMAX normal property\n",
    "    content_tag = soup.find('div', class_='property-details-body fullwidth-content-container clearfix')\n",
    "    price = get_price(soup)\n",
    "    street, city, state, zipcode = get_address(content_tag)\n",
    "    sidict = get_sideinfo(content_tag)\n",
    "    listid = access_dict(sidict, 'Listing ID')\n",
    "    listtype = access_dict(sidict, 'Listing Type')\n",
    "    bedrooms = access_dict(sidict, 'Bedrooms')\n",
    "    bathrooms = access_dict(sidict, 'Bathrooms')\n",
    "    sqft = access_dict(sidict, 'House Size')\n",
    "    lotsf = access_dict(sidict, 'Lot Size')\n",
    "    waterfront = access_dict(sidict, 'Waterfront')\n",
    "    liststatus = access_dict(sidict, 'Listing Status')\n",
    "    yrbuilt = access_dict(sidict, 'Year Built')\n",
    "    county = access_dict(sidict, 'County')\n",
    "    halfbath = access_dict(sidict, 'Half Bath')\n",
    "    subdivision = access_dict(sidict, 'Subdivision')\n",
    "    cooling = access_dict(sidict, 'Cooling')\n",
    "    ac = access_dict(sidict, 'Air Conditioning')\n",
    "    appliances = access_dict(sidict, 'Appliances')\n",
    "    rooms = access_dict(sidict, 'Rooms')\n",
    "    laundry = access_dict(sidict, 'Laundry')\n",
    "    taxes = access_dict(sidict, 'Taxes')\n",
    "    luxurious = 'No'\n",
    "\n",
    "    unit = [\n",
    "        street,\n",
    "        city,\n",
    "        state,\n",
    "        zipcode,\n",
    "        bathrooms,\n",
    "        bedrooms,\n",
    "        rooms,\n",
    "        waterfront,\n",
    "        cooling,\n",
    "        ac,\n",
    "        appliances,\n",
    "        laundry,\n",
    "        sqft,\n",
    "        price,\n",
    "        taxes,\n",
    "        listtype,\n",
    "        listid,\n",
    "        lotsf,\n",
    "        liststatus,\n",
    "        yrbuilt,\n",
    "        county,\n",
    "        halfbath,\n",
    "        subdivision,\n",
    "        luxurious,\n",
    "    ]\n",
    "\n",
    "    return unit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remax_collection(soup):\n",
    "    # REMAX luxurious property \n",
    "    price = get_price_normal(soup)\n",
    "    content_tag = soup.find('div', class_='property-details--details')\n",
    "    street, city, state, zipcode = get_address(content_tag)\n",
    "    sidict = get_sideinfo(content_tag)\n",
    "    listid = access_dict(sidict, 'Listing ID')\n",
    "    listtype = access_dict(sidict, 'Listing Type')\n",
    "    bedrooms = access_dict(sidict, 'Bedrooms')\n",
    "    bathrooms = access_dict(sidict, 'Bathrooms')\n",
    "    sqft = access_dict(sidict, 'House Size')\n",
    "    lotsf = access_dict(sidict, 'Lot Size')\n",
    "    waterfront = access_dict(sidict, 'Waterfront')\n",
    "    liststatus = access_dict(sidict, 'Listing Status')\n",
    "    yrbuilt = access_dict(sidict, 'Year Built')\n",
    "    county = access_dict(sidict, 'County')\n",
    "    halfbath = access_dict(sidict, 'Half Bath')\n",
    "    subdivision = access_dict(sidict, 'Subdivision')\n",
    "    cooling = access_dict(sidict, 'Cooling')\n",
    "    ac = access_dict(sidict, 'Air Conditioning')\n",
    "    appliances = access_dict(sidict, 'Appliances')\n",
    "    rooms = access_dict(sidict, 'Rooms')\n",
    "    laundry = access_dict(sidict, 'Laundry')\n",
    "    taxes = access_dict(sidict, 'Taxes')\n",
    "    luxurious = 'Yes'\n",
    "\n",
    "    unit = [\n",
    "        street,\n",
    "        city,\n",
    "        state,\n",
    "        zipcode,\n",
    "        bathrooms,\n",
    "        bedrooms,\n",
    "        rooms,\n",
    "        waterfront,\n",
    "        cooling,\n",
    "        ac,\n",
    "        appliances,\n",
    "        laundry,\n",
    "        sqft,\n",
    "        price,\n",
    "        taxes,\n",
    "        listtype,\n",
    "        listid,\n",
    "        lotsf,\n",
    "        liststatus,\n",
    "        yrbuilt,\n",
    "        county,\n",
    "        halfbath,\n",
    "        subdivision,\n",
    "        luxurious,\n",
    "    ]\n",
    "\n",
    "    return unit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remax_apt(soup, content_tag):\n",
    "    price = get_price_normal(soup)\n",
    "    street, city, state, zipcode = get_address(content_tag)\n",
    "    sidict = get_sideinfo(content_tag)\n",
    "    listid = access_dict(sidict, 'Listing ID')\n",
    "    listtype = access_dict(sidict, 'Listing Type')\n",
    "    bedrooms = access_dict(sidict, 'Bedrooms')\n",
    "    bathrooms = access_dict(sidict, 'Bathrooms')\n",
    "    sqft = access_dict(sidict, 'House Size')\n",
    "    lotsf = access_dict(sidict, 'Lot Size')\n",
    "    waterfront = access_dict(sidict, 'Waterfront')\n",
    "    liststatus = access_dict(sidict, 'Listing Status')\n",
    "    yrbuilt = access_dict(sidict, 'Year Built')\n",
    "    county = access_dict(sidict, 'County')\n",
    "    halfbath = access_dict(sidict, 'Half Bath')\n",
    "    subdivision = access_dict(sidict, 'Subdivision')\n",
    "    cooling = access_dict(sidict, 'Cooling')\n",
    "    ac = access_dict(sidict, 'Air Conditioning')\n",
    "    appliances = access_dict(sidict, 'Appliances')\n",
    "    rooms = access_dict(sidict, 'Rooms')\n",
    "    laundry = access_dict(sidict, 'Laundry')\n",
    "    taxes = access_dict(sidict, 'Taxes')\n",
    "    luxurious = 'Yes'\n",
    "\n",
    "    unit = [\n",
    "        street,\n",
    "        city,\n",
    "        state,\n",
    "        zipcode,\n",
    "        bathrooms,\n",
    "        bedrooms,\n",
    "        rooms,\n",
    "        waterfront,\n",
    "        cooling,\n",
    "        ac,\n",
    "        appliances,\n",
    "        laundry,\n",
    "        sqft,\n",
    "        price,\n",
    "        taxes,\n",
    "        listtype,\n",
    "        listid,\n",
    "        lotsf,\n",
    "        liststatus,\n",
    "        yrbuilt,\n",
    "        county,\n",
    "        halfbath,\n",
    "        subdivision,\n",
    "        luxurious,\n",
    "    ]\n",
    "\n",
    "    return unit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_lux(soup):\n",
    "    try:\n",
    "        is_lux = False\n",
    "        \n",
    "        lux_tag = soup.find('span', attrs={\n",
    "            'itemprop': 'name',\n",
    "            'class': 'js-stateformatted'\n",
    "        })\n",
    "        \n",
    "        lux = lux_tag.get_text()\\\n",
    "                     .strip()\\\n",
    "                     .lower()\n",
    "        \n",
    "        if 'luxury' in lux:\n",
    "            is_lux = True\n",
    "        return is_lux\n",
    "    except:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_apt_info(apt_url):\n",
    "    overhead = 'https://www.remax.com'\n",
    "    response = requests.get(overhead+apt_url)\n",
    "    results = response.content  \n",
    "    if not response.status_code == 404:\n",
    "        soup = BeautifulSoup(results, 'lxml')\n",
    "        is_lux = check_lux(soup)\n",
    "        if is_lux:\n",
    "            content_tag = soup.find('div', class_='property-details--details')\n",
    "        else:\n",
    "            content_tag = soup.find('div', class_='property-details-body fullwidth-content-container clearfix')\n",
    "        apt_info = remax_apt(soup, content_tag)\n",
    "    return apt_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['767 N 24TH ST',\n",
       " 'Philadelphia',\n",
       " 'PA',\n",
       " '19130',\n",
       " 2.0,\n",
       " 3.0,\n",
       " 'Breakfast Room,  Dining Room,  Kitchen,  Laundry,  Living Room',\n",
       " 'No',\n",
       " 'Central A/C',\n",
       " 'Yes',\n",
       " 'Built-In Microwave,  Cooktop,  Dishwasher,  Oven/Range - Gas,  Range Hood',\n",
       " None,\n",
       " 2128.0,\n",
       " 710000.0,\n",
       " 7268.0,\n",
       " 'Condo/Townhome',\n",
       " 'PAPH849490',\n",
       " 1742.0,\n",
       " 'Active',\n",
       " 1920.0,\n",
       " 'Philadelphia',\n",
       " None,\n",
       " 'Fairmount',\n",
       " 'Yes']"
      ]
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_apt_info(normal_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1417 N 8TH ST',\n",
       " 'Philadelphia',\n",
       " 'PA',\n",
       " '19122',\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " 'No',\n",
       " 'Central A/C',\n",
       " 'Yes',\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " 850000.0,\n",
       " 489.0,\n",
       " 'Condo/Townhome',\n",
       " 'PAPH850180',\n",
       " 1742.0,\n",
       " 'Active',\n",
       " 2020.0,\n",
       " 'Philadelphia County',\n",
       " None,\n",
       " 'Ludlow',\n",
       " 'Yes']"
      ]
     },
     "execution_count": 306,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_apt_info(collection_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
