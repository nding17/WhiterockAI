{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_buy_webpage(city, \n",
    "                    state, \n",
    "                    pg_num, \n",
    "                    htype=[\n",
    "                        'house',\n",
    "                        'multi-family',\n",
    "                    ]):\n",
    "    \n",
    "    overhead = 'https://www.trulia.com'\n",
    "    dangle = 'for_sale'\n",
    "\n",
    "    city = city.title()\\\n",
    "               .replace(' ', '_')\n",
    "    state = state.upper()\n",
    "\n",
    "    dict_alias = {\n",
    "        'house': 'SINGLE-FAMILY_HOME',\n",
    "        'condo': 'APARTMENT,CONDO,COOP',\n",
    "        'townhouse': 'TOWNHOUSE',\n",
    "        'multi-family': 'MULTI-FAMILY',\n",
    "        'land': 'LOT%7CLAND',\n",
    "        'mobile/manufactured': 'MOBILE%7CMANUFACTURED',\n",
    "        'other': 'UNKNOWN',\n",
    "    }\n",
    "\n",
    "    aliases = [dict_alias[h] for h in htype]\n",
    "    houses = ','.join(aliases)\n",
    "\n",
    "    webpage = f'{overhead}/{dangle}/{city},{state}/{houses}_type/{pg_num}_p/'\n",
    "    return webpage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = get_buy_webpages('philadelphia', 'pa', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_buy_apt_urls_per_page(city,\n",
    "                              state,\n",
    "                              pg_num,\n",
    "                              htype=['house', \n",
    "                                     'multi-family']):\n",
    "\n",
    "    webpage = get_buy_webpage(city, state, pg_num, htype)\n",
    "    \n",
    "    # Here we added User-Agent to the header of our request \n",
    "    # It is because sometimes the web server will check the\n",
    "    # different fields of the header to block robot scrapers\n",
    "    # User-Agent is the most common one because it is specific \n",
    "    # to your browser.\n",
    "    headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_3) \\\n",
    "            AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36'}\n",
    "    response = requests.get(webpage, headers=headers)\n",
    "    results = response.content\n",
    "\n",
    "    if not response.status_code == 404:\n",
    "        soup = BeautifulSoup(results, 'lxml')\n",
    "        apt_tags = soup.find_all('div', class_='PropertyCard__PropertyCardContainer-sc-1ush98q-0 gsDQZj Box-sc-8ox7qa-0 jIGxjA')\n",
    "        apt_link_tags = [tag.find('a') for tag in apt_tags]\n",
    "        apt_urls = [tag['href'] for tag in apt_link_tags]\n",
    "        return len(apt_urls)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_buy_apt_urls_per_page('philadelphia', 'pa', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
