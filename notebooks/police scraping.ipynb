{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import time\n",
    "import os\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from webdriver_manager.chrome import ChromeDriverManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _random_user_agent():\n",
    "    \"\"\"\n",
    "    A helper function to generate a random header to \n",
    "    avoid getting blocked by the website\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    None\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "    a random user agent \n",
    "\n",
    "    >>> _random_user_agent()\n",
    "    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_3) \\\n",
    "                AppleWebKit/537.36 (KHTML, like Gecko) \\\n",
    "                Chrome/58.0.3029.110 Safari/537.36'\n",
    "    \"\"\"\n",
    "    try:\n",
    "        ua = UserAgent()\n",
    "        return ua.random\n",
    "    except:\n",
    "        default_ua = 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_3) \\\n",
    "                AppleWebKit/537.36 (KHTML, like Gecko) \\\n",
    "                Chrome/58.0.3029.110 Safari/537.36'\n",
    "        return default_ua\n",
    "\n",
    "def _get_soup(url):\n",
    "    \"\"\"\n",
    "    This is a helper function that will automatically generate a \n",
    "    BeautifulSoup object based on the given URL of the apartment \n",
    "    webpage\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    url : str\n",
    "        the URL of a specific apartment or a general website \n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    soup : bs4.BeautifulSoup\n",
    "        a scraper for a specified webpage\n",
    "    \"\"\"\n",
    "\n",
    "    # generate a random header \n",
    "    headers = {'User-Agent': _random_user_agent()}\n",
    "    # send a request and get the soup\n",
    "    response = requests.get(url, headers=headers)\n",
    "    results = response.content\n",
    "    if not response.status_code == 404:\n",
    "        soup = BeautifulSoup(results, 'lxml')\n",
    "    return soup\n",
    "\n",
    "def _soup_attempts(url, total_attempts=5):\n",
    "\n",
    "    \"\"\"\n",
    "    A helper function that will make several attempts\n",
    "    to obtain a soup to avoid getting blocked\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    url : str\n",
    "        the URL of a specific apartment or a general website \n",
    "\n",
    "    total_attempts: int\n",
    "        the number of attempts you want to try to obtain the \n",
    "        soup before you already give up. Default is 5 attempts\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    soup : bs4.BeautifulSoup\n",
    "        a scraper for a specified webpage        \n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    soup = _get_soup(url)\n",
    "\n",
    "    # if we get the soup with the first attempt\n",
    "    if soup:\n",
    "        return soup\n",
    "    # if we don't get the soup during our first\n",
    "    # attempt\n",
    "    else:\n",
    "        attempts = 0\n",
    "        while attempts < total_attempts:\n",
    "            # put the program idle to avoid detection\n",
    "            time.sleep(3)\n",
    "            soup = self._get_soup(url)\n",
    "            if soup:\n",
    "                return soup\n",
    "        # time to give up, try to find what's going on \n",
    "        raise ValueError(f'FAILED to get soup for apt url {url}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "police_url = 'https://www.cityprotect.com/map/list/incidents?toUpdateDate=12%2F18%2F2019&fromUpdateDate=11%2F18%2F2019&pageSize=2000&parentIncidentTypeIds=149,150,148,8,97,104,165,98,100,179,178,180,101,99,103,163,168,166,12&zoomLevel=16&latitude=39.94761343841498&longitude=-75.15636979615388&days=1,2,3,4,5,6,7&startHour=0&endHour=24&timezone=-05:00'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _build_chrome_options():\n",
    "        chrome_options = webdriver.ChromeOptions()\n",
    "        chrome_options.accept_untrusted_certs = True\n",
    "        chrome_options.assume_untrusted_cert_issuer = True\n",
    "        \n",
    "        # chrome configuration\n",
    "        # More: https://github.com/SeleniumHQ/docker-selenium/issues/89\n",
    "        # And: https://github.com/SeleniumHQ/docker-selenium/issues/87\n",
    "        chrome_options.add_argument(\"--no-sandbox\")\n",
    "        chrome_options.add_argument(\"--disable-impl-side-painting\")\n",
    "        chrome_options.add_argument(\"--disable-setuid-sandbox\")\n",
    "        chrome_options.add_argument(\"--disable-seccomp-filter-sandbox\")\n",
    "        chrome_options.add_argument(\"--disable-breakpad\")\n",
    "        chrome_options.add_argument(\"--disable-client-side-phishing-detection\")\n",
    "        chrome_options.add_argument(\"--disable-cast\")\n",
    "        chrome_options.add_argument(\"--disable-cast-streaming-hw-encoding\")\n",
    "        chrome_options.add_argument(\"--disable-cloud-import\")\n",
    "        chrome_options.add_argument(\"--disable-popup-blocking\")\n",
    "        chrome_options.add_argument(\"--ignore-certificate-errors\")\n",
    "        chrome_options.add_argument(\"--disable-session-crashed-bubble\")\n",
    "        chrome_options.add_argument(\"--disable-ipv6\")\n",
    "        chrome_options.add_argument(\"--allow-http-screen-capture\")\n",
    "        chrome_options.add_argument(\"--start-maximized\")\n",
    "        chrome_options.add_argument('--lang=es')\n",
    "\n",
    "        return chrome_options\n",
    "\n",
    "def _get_browser():\n",
    "    \"\"\"\n",
    "    A helper function to get the selenium browser in order \n",
    "    to perform the scraping tasks \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    chromedriver : str\n",
    "        the path to the location of the chromedriver \n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    browser : webdriver.chrome\n",
    "        a chrome web driver \n",
    "\n",
    "    wait : WebDriverWait\n",
    "        this is wait object that allows the program to hang around for a period\n",
    "        of time since we need some time to listen to the server \n",
    "\n",
    "    \"\"\"\n",
    "    options = _build_chrome_options()\n",
    "\n",
    "    browser = webdriver.Chrome(ChromeDriverManager().install(), chrome_options=options)\n",
    "    browser.get(police_url)\n",
    "    wait = WebDriverWait(browser, 10) # maximum wait time is 20 seconds \n",
    "    return browser, wait"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Looking for [chromedriver 79.0.3945.36 mac64] driver in cache \n",
      "File found in cache by path [/Users/nailiding/.wdm/drivers/chromedriver/79.0.3945.36/mac64/chromedriver]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nailiding/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:50: DeprecationWarning: use options instead of chrome_options\n"
     ]
    }
   ],
   "source": [
    "browser, wait = _get_browser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 收集需要的信息，返回list\n",
    "def collect(elem):\n",
    "    case_number = elem.find_element_by_xpath(\"//*[@id='incident-case-number']\").text\n",
    "    title = elem.find_element_by_xpath(\"//*[@class='incident-title']\").text\n",
    "    address = elem.find_element_by_xpath(\"//*[@class='incident-subtitle']\").text\n",
    "    \n",
    "    date = elem.find_element_by_xpath(\"//*[@id='incident-date']\").text\n",
    "    time = elem.find_element_by_xpath(\"//*[@id='incident-time']\").text  \n",
    "    agency = elem.find_element_by_xpath(\"//*[@id='incident-agency']\").text\n",
    "    description = elem.find_element_by_xpath(\"//*[@id='incident-description']\").text\n",
    "    \n",
    "    return [case_number, title, address, date, time, agency, description]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def scrape(cases, all_case_number, browser):\n",
    "    scrollHeight = browser.execute_script('return document.getElementById(\"incidentsList\").scrollHeight')\n",
    "    i=0\n",
    "    \n",
    "    while True:\n",
    "        # 若该区域内没有incident，find_element_by_xpath会报错\n",
    "        try:\n",
    "            test = browser.find_element_by_xpath(\"//*[@id='incidentsList']/div[1]/ce-incident-item\")\n",
    "            \n",
    "            # 每次向下 scroll 50，收集此时左边列表中的第一个\n",
    "            # 如果已经存在，则跳过\n",
    "            while 50*(i) <= scrollHeight:\n",
    "                js = f'document.getElementById(\"incidentsList\").scrollTop=50*{i}'\n",
    "                browser.execute_script(js)\n",
    "                if i == 0:\n",
    "                    time.sleep(5)\n",
    "\n",
    "                elem = browser.find_element_by_xpath(\"//*[@id='incidentsList']/div[1]/ce-incident-item\")\n",
    "                infomation = collect(elem)\n",
    "                if infomation[0] not in all_case_number:\n",
    "                    cases.append(infomation)\n",
    "                    all_case_number.append(infomation[0])\n",
    "                i += 1\n",
    "\n",
    "            # scroll到最底部，收集此时左边列表中的除了第一个之外的剩下所有\n",
    "            elem = browser.find_element_by_xpath(\"//ce-incident-item[@class='ng-star-inserted']\")\n",
    "            final_elems = elem.find_elements_by_xpath(\"//span[@id='incident-case-number']\")\n",
    "\n",
    "            case_numbers = elem.find_elements_by_xpath(\"//span[@id='incident-case-number']\")\n",
    "            titles = elem.find_elements_by_xpath(\"//span[@class='incident-title']\")\n",
    "            addresses = elem.find_elements_by_xpath(\"//span[@class='incident-subtitle']\")\n",
    "            dates = elem.find_elements_by_xpath(\"//span[@id='incident-date']\")\n",
    "            times = elem.find_elements_by_xpath(\"//span[@id='incident-time']\")  \n",
    "            agencies = elem.find_elements_by_xpath(\"//span[@id='incident-agency']\")\n",
    "            descriptions = elem.find_elements_by_xpath(\"//span[@id='incident-description']\")\n",
    "\n",
    "            for i in range(len(final_elems)):\n",
    "                if case_numbers[i].text not in all_case_number:\n",
    "                    all_case_number.append(case_numbers[i].text)\n",
    "                    cases.append([case_numbers[i].text, titles[i].text, \\\n",
    "                                  addresses[i].text, dates[i].text, times[i].text, \\\n",
    "                              agencies[i].text, descriptions[i].text])\n",
    "            break\n",
    "        except:\n",
    "            print(\"Oops! This district has no incident!\")\n",
    "            break\n",
    "\n",
    "    return cases, all_case_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def move(browser, direction, times):\n",
    "    if direction == 'left':\n",
    "        for i in range(500*times):\n",
    "            browser.find_element_by_xpath(\"//*[@id='mapMainContainer']/ce-map-wrapper/div/uwm-universal-web-map/div/div[1]/div[1]\")\\\n",
    "                    .send_keys(Keys.LEFT)\n",
    "            \n",
    "    elif direction == 'right': \n",
    "        for i in range(500*times):\n",
    "            browser.find_element_by_xpath(\"//*[@id='mapMainContainer']/ce-map-wrapper/div/uwm-universal-web-map/div/div[1]/div[1]\")\\\n",
    "                    .send_keys(Keys.RIGHT)\n",
    "    \n",
    "    elif direction == 'up':\n",
    "        for i in range(400*times):\n",
    "            browser.find_element_by_xpath(\"//*[@id='mapMainContainer']/ce-map-wrapper/div/uwm-universal-web-map/div/div[1]/div[1]\")\\\n",
    "                    .send_keys(Keys.UP)   \n",
    "    \n",
    "    elif direction == 'down':\n",
    "        for i in range(400*times):\n",
    "            browser.find_element_by_xpath(\"//*[@id='mapMainContainer']/ce-map-wrapper/div/uwm-universal-web-map/div/div[1]/div[1]\")\\\n",
    "                    .send_keys(Keys.DOWN)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_map(browser, left=1, right=1, up=1, down=1):\n",
    "    \n",
    "    cases = []\n",
    "    all_case_number = []\n",
    "    # move to the up-left corner\n",
    "    move(browser, 'left', left)\n",
    "    move(browser, 'up', up)\n",
    "    time.sleep(5)\n",
    "    \n",
    "    for i in range(up+down+1):\n",
    "        if i%2 == 0:\n",
    "            for j in range(left+right):\n",
    "                cases, all_case_number = scrape(cases, all_case_number, browser)\n",
    "                move(browser, 'right', 1)\n",
    "                time.sleep(5)\n",
    "        elif i%2 == 1:\n",
    "            for j in range(left+right):\n",
    "                cases, all_case_number = scrape(cases, all_case_number, browser)\n",
    "                move(browser, 'left', 1)\n",
    "                time.sleep(5) \n",
    "        cases, all_case_number = scrape(cases, all_case_number, browser)\n",
    "        move(browser, 'down', 1)\n",
    "        time.sleep(5) \n",
    "        \n",
    "    return cases, all_case_number\n",
    "    \n",
    "# 先移动到左上角\n",
    "# for i in range(up+down+1):\n",
    "#     如果 i 是偶数:\n",
    "#         for j in range(left+right):\n",
    "#             scrape 当前格，向右移动一步\n",
    "#     如果 i 是奇数:\n",
    "#         for j in range(left+right):\n",
    "#             scrape 当前格，向左移动一步\n",
    "#     scrape当前格，向下移一步"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Dec 19 18:57:16 2019\n",
      "Oops! This district has no incident!\n",
      "Oops! This district has no incident!\n",
      "Oops! This district has no incident!\n",
      "Oops! This district has no incident!\n",
      "Thu Dec 19 19:16:41 2019\n"
     ]
    }
   ],
   "source": [
    "print(time.ctime())\n",
    "cases, all_case_number = scrape_map(browser, left=2, right=2, up=2, down=2)\n",
    "print(time.ctime())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1242"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "868"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_case_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['201977006461',\n",
       "  'Theft',\n",
       "  'D & E CHECKPOINT',\n",
       "  '11/16/2019',\n",
       "  '5PM',\n",
       "  'Philadelphia Police Department',\n",
       "  ''],\n",
       " ['201909048372',\n",
       "  'Theft',\n",
       "  '1600 Block WALNUT ST',\n",
       "  '11/16/2019',\n",
       "  '11PM',\n",
       "  'Philadelphia Police Department',\n",
       "  ''],\n",
       " ['201906059679',\n",
       "  'Theft',\n",
       "  'N 12TH ST',\n",
       "  '11/17/2019',\n",
       "  '12AM',\n",
       "  'Philadelphia Police Department',\n",
       "  ''],\n",
       " ['201909048436',\n",
       "  'Theft',\n",
       "  '1500 Block CHESTNUT ST',\n",
       "  '11/17/2019',\n",
       "  '1PM',\n",
       "  'Philadelphia Police Department',\n",
       "  ''],\n",
       " ['201909048443',\n",
       "  'Theft',\n",
       "  '1500 Block LOCUST ST',\n",
       "  '11/17/2019',\n",
       "  '2PM',\n",
       "  'Philadelphia Police Department',\n",
       "  ''],\n",
       " ['201909048441',\n",
       "  'Theft from Vehicle',\n",
       "  '100 Block N 21ST ST',\n",
       "  '11/17/2019',\n",
       "  '2PM',\n",
       "  'Philadelphia Police Department',\n",
       "  ''],\n",
       " ['201909048452',\n",
       "  'Theft',\n",
       "  '1700 Block WALNUT ST',\n",
       "  '11/17/2019',\n",
       "  '3PM',\n",
       "  'Philadelphia Police Department',\n",
       "  ''],\n",
       " ['201909048464',\n",
       "  'Theft',\n",
       "  '100 Block S 21ST ST',\n",
       "  '11/17/2019',\n",
       "  '5PM',\n",
       "  'Philadelphia Police Department',\n",
       "  ''],\n",
       " ['201909048475',\n",
       "  'Theft',\n",
       "  '1700 Block CHESTNUT ST',\n",
       "  '11/17/2019',\n",
       "  '7PM',\n",
       "  'Philadelphia Police Department',\n",
       "  ''],\n",
       " ['201906060157',\n",
       "  'Theft',\n",
       "  '1300 Block MARKET ST',\n",
       "  '11/19/2019',\n",
       "  '3PM',\n",
       "  'Philadelphia Police Department',\n",
       "  '']]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cases[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "metadata": {},
   "outputs": [],
   "source": [
    "# move down\n",
    "for i in range(400):\n",
    "    browser.find_element_by_xpath(\"//*[@id='mapMainContainer']/ce-map-wrapper/div/uwm-universal-web-map/div/div[1]/div[1]\")\\\n",
    "            .send_keys(Keys.DOWN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "metadata": {},
   "outputs": [],
   "source": [
    "# move up\n",
    "for i in range(400):\n",
    "    browser.find_element_by_xpath(\"//*[@id='mapMainContainer']/ce-map-wrapper/div/uwm-universal-web-map/div/div[1]/div[1]\")\\\n",
    "            .send_keys(Keys.UP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 514,
   "metadata": {},
   "outputs": [],
   "source": [
    "# move left\n",
    "for i in range(500):\n",
    "    browser.find_element_by_xpath(\"//*[@id='mapMainContainer']/ce-map-wrapper/div/uwm-universal-web-map/div/div[1]/div[1]\")\\\n",
    "            .send_keys(Keys.LEFT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "metadata": {},
   "outputs": [],
   "source": [
    "# move right\n",
    "for i in range(500):\n",
    "    browser.find_element_by_xpath(\"//*[@id='mapMainContainer']/ce-map-wrapper/div/uwm-universal-web-map/div/div[1]/div[1]\")\\\n",
    "            .send_keys(Keys.RIGHT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape(cases, all_case_number, browser):\n",
    "    \n",
    "    scrollHeight = browser.execute_script('return document.getElementById(\"incidentsList\").scrollHeight')\n",
    "    i=0\n",
    "\n",
    "    while 50*(i) <= scrollHeight:\n",
    "        js = f'document.getElementById(\"incidentsList\").scrollTop=50*{i}'\n",
    "        browser.execute_script(js)\n",
    "        if i == 0:\n",
    "            time.sleep(5)\n",
    "\n",
    "        elem = browser.find_element_by_xpath(\"//*[@id='incidentsList']/div[1]/ce-incident-item\")\n",
    "        infomation = collect(elem)\n",
    "        if infomation[0] not in all_case_number:\n",
    "            cases.append(infomation)\n",
    "            all_case_number.append(infomation[0])\n",
    "        i += 1\n",
    "\n",
    "\n",
    "    elem = browser.find_element_by_xpath(\"//ce-incident-item[@class='ng-star-inserted']\")\n",
    "    final_elems = elem.find_elements_by_xpath(\"//span[@id='incident-case-number']\")\n",
    "\n",
    "    case_numbers = elem.find_elements_by_xpath(\"//span[@id='incident-case-number']\")\n",
    "    titles = elem.find_elements_by_xpath(\"//span[@class='incident-title']\")\n",
    "    addresses = elem.find_elements_by_xpath(\"//span[@class='incident-subtitle']\")\n",
    "    dates = elem.find_elements_by_xpath(\"//span[@id='incident-date']\")\n",
    "    times = elem.find_elements_by_xpath(\"//span[@id='incident-time']\")  \n",
    "    agencies = elem.find_elements_by_xpath(\"//span[@id='incident-agency']\")\n",
    "    descriptions = elem.find_elements_by_xpath(\"//span[@id='incident-description']\")\n",
    "\n",
    "    for i in range(len(final_elems)):\n",
    "        if case_numbers[i].text not in all_case_number:\n",
    "            all_case_number.append(case_numbers[i].text)\n",
    "            cases.append([case_numbers[i].text, titles[i].text, \\\n",
    "                          addresses[i].text, dates[i].text, times[i].text, \\\n",
    "                      agencies[i].text, descriptions[i].text])\n",
    "\n",
    "    return cases, all_case_number"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
